{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Title: Chapter 9 Solutions\n",
    "\n",
    "Date: May 10, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is a continuous variable?\n",
    "    - <span style=\"color:blue\">Continuous variables are numerical data, such as \"age\", that can be directly fed to the model since you can add and multiply them directly. </span>\n",
    "2. What is a categorical variable?\n",
    "    - <span style=\"color:blue\">Categorical variables contain a number of discrete levels, such as \"movie ID,\" for which addition and multiplication don't have meaning even if they're stored as numbers.  </span>\n",
    "3. Provide two of the words that are used for the possible values of a categorical variable.\n",
    "    - <span style=\"color:blue\">Possible values of a categorical variable can be called its \"levels\", \"categories\" or \"classes.\" </span>\n",
    "4. What is a \"dense layer\"?\n",
    "    - <span style=\"color:blue\">The \"dense layer\" is equivalent to an ordinary linear layer that's placed after every one-hot-encoded input layer. </span>\n",
    "5. How do entity embeddings reduce memory usage and speed up neural networks?\n",
    "    - <span style=\"color:blue\">When datasets have high cardinality features, it can be memory intensive to store them as one hot encodings that are likely sparse. Entity embeddings allow more memory efficient and dense representations of the data which therefore helps speed up neural networks. </span>\n",
    "6. What kinds of datasets are entity embeddings especially useful for?\n",
    "    - <span style=\"color:blue\">Entity embeddings are especially useful for datasets with lots of high cardinality features where other methods tend to overfit. It is also helpful for visualizing categorial data and for data clustering as the entity embeddings define a distance measure for categorical variables.</span>\n",
    "7. What are the two main families of machine learning algorithms?\n",
    "    - <span style=\"color:blue\">Ensembles of decision trees which are mainly used for structured data and multilayered neural networks learned with stochastic gradient descent, mainly used for unstructured data such as audio, images and natural language </span>\n",
    "8. Why do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\n",
    "    - <span style=\"color:blue\">These type of columns are called ordinal columns which refers to strings that have a natural ordering such as \"small\", \"medium\", \"large.\" To do this in Pandas, we first set the dataframe column astype 'category.' Then we pass in an ordered list to set_categories with ordered set to True to let Pandas know about a suitable ordering of the column levels. </span>\n",
    "9. Summarize what a decision tree algorithm does.\n",
    "    - <span style=\"color:blue\">The basic idea of what a decision tree algorithm does is to determine how to group the data based on “questions” that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\n",
    "\n",
    "    <span style=\"color:blue\"> - Loop through each column of the dataset in turn </span>\n",
    "    <span style=\"color:blue\"> - For each column, loop through each possible level of that column in turn </span>\n",
    "    <span style=\"color:blue\"> - Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable) </span>\n",
    "    <span style=\"color:blue\"> - Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group </span>\n",
    "    <span style=\"color:blue\"> - After looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model </span>\n",
    "    <span style=\"color:blue\"> - We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group </span>\n",
    "    <span style=\"color:blue\"> - Continue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it. </span>\n",
    "10. Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\n",
    "    - <span style=\"color:blue\">Some dates are qualitatively different from others. We might want our model to make decisions based on that date's day of the week, on whether a day is a holiday, on what month it is in and so forth. Therefore, we replace every date column with a set of date metadata columnstjat we think will be useful. </span>\n",
    "11. Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\n",
    "    - <span style=\"color:blue\">No, as the goal of the model is to predict the future. Therefore we will need a validation set to be later in time than the training set. </span>\n",
    "12. What is pickle and what is it useful for?\n",
    "    - <span style=\"color:blue\">It allows you to save nearly any Python object. For example, since preprocessing datat can take some time, we can save our preprocessed TabularPandas object at a given point so tis easy to continue our work another day without rerunning previous steps. </span>\n",
    "13. How are mse, samples, and values calculated in the decision tree drawn in this chapter?\n",
    "    - <span style=\"color:blue\">By traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group. </span>\n",
    "14. How do we deal with outliers, before building a decision tree?\n",
    "    - <span style=\"color:blue\">Outliers can sometimes make visualizing decision trees more difficult. For example, with year 1000 as a placeholder for missing values, we weren't able to get a good visualization of the distribution of the YearMade data. By changing the values to 1950, the distribution was much more apparent through the tree visualization; it also doesn't change the result of the model in any significant way which demonstrates the resilience of decision trees to data issues. </span>\n",
    "15. How do we handle categorical variables in a decision tree?\n",
    "    - <span style=\"color:blue\">We don't have to do anything. During training, the decision tree algorithm will automatically make splits that increase homogenity within a subgroup, which hones in on the intrinsic meaning of the different levels of the variable.  </span>\n",
    "16. What is bagging?\n",
    "    - <span style=\"color:blue\">Bagging is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The process is as follows: 1) Randomly choose a subset of the rows of your data 2) Train a model using this subset 3) Save that model, and tehn return to step 1 a few times 4) You will end up with multiple trained models. To make a prediction, predict using all of the models and then take the average of each of those model's predictions. While each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will be uncorrelated with each other. Different models will make different errors and the average of those errors is zero. Therefore if we take the average of all the model's predictions, we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. </span>\n",
    "17. What is the difference between max_samples and max_features when creating a random forest?\n",
    "    - <span style=\"color:blue\">Max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point. </span>\n",
    "18. If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\n",
    "    - <span style=\"color:blue\">x </span>\n",
    "19. In the section \"Creating a Random Forest\", just after <<max_features>>, why did preds.mean(0) give the same result as our random forest?\n",
    "    - <span style=\"color:blue\">Because we are taking the average of the predictions from each individual tree in our forest through the estimators_ attribute.</span>\n",
    "20. What is \"out-of-bag-error\"?\n",
    "    - <span style=\"color:blue\">A way of measuring prediction error in the training dataset by validating only on  rows that were not included in the training </span>\n",
    "21. Make a list of reasons why a model's validation set error might be worse than the OOB error. How could you test your hypotheses?\n",
    "    - <span style=\"color:blue\">x </span>\n",
    "22. Explain why random forests are well suited to answering each of the following question:\n",
    "    - How confident are we in our predictions using a particular row of data?\n",
    "    <span style=\"color:blue\">Measure standard deviation of predictions across the trees; we would want to be more cautious of using results for rows where trees give very different results (higher standard dev) compared to cases where they are more consistent (lower standard dev) </span> \n",
    "    - For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n",
    "    <span style=\"color:blue\">Use the feature_importances_ attribute which loops through each tree and recursively explores each branch looking at what feature was used for that split, and how much the model improves as a result of that split </span> \n",
    "    -  Which columns are the strongest predictors?\n",
    "    <span style=\"color:blue\">Features with the highest importance scores. </span> \n",
    "    - How do predictions vary as we vary these columns?\n",
    "    <span style=\"color:blue\">Partial dependence plots allows us to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? </span> \n",
    "23. What's the purpose of removing unimportant variables?\n",
    "    - <span style=\"color:blue\">Simpler, more interpretable models are easier to roll out and maintain. By honing in on the important variables, we will also be able to study them more in depth. </span>\n",
    "24. What's a good type of plot for showing tree interpreter results?\n",
    "    - <span style=\"color:blue\">A waterfall plot shows how the positive and negative contributions from all the independent variables sum up to create the final prediction. </span>\n",
    "25. What is the \"extrapolation problem\"?\n",
    "    - <span style=\"color:blue\">Random forests are not able to extrapolate outside of the types of data they have seen. </span>\n",
    "26. How can you tell if your test or validation set is distributed in a different way than your training set?\n",
    "    - <span style=\"color:blue\">We can combine our training and validation sets, create a dependent variable that represents which dataset each row comes from, build a random forest using that data and get its feature importance. By looking at the feature importance, we can see which columns differ significantly between the datasets. </span>\n",
    "27. Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\n",
    "    - <span style=\"color:blue\">This is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. </span>\n",
    "28. What is \"boosting\"?\n",
    "    - <span style=\"color:blue\">We train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. Each new tree will be attempting to fit the error of all of the previous trees combined. We continually create new resideuals by subtracting the predictions of each new tree from the residuals of the previous trees. As a result, the residuals increasingly decrease. To make predictions with an ensemple of boosted trees, we calculate the predictions from each tree and then add them all together. </span>\n",
    "29. How could we use embeddings with a random forest? Would we expect this to help?\n",
    "    - <span style=\"color:blue\">Entity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. </span>\n",
    "30. Why might we not always use a neural net for tabular modeling?\n",
    "    - <span style=\"color:blue\">Neural networsk take the longest time to train and require extra preprocessing such as normalization. They are also highly sensitive to hyperparameter settings. Therefore it is best to start with a random forest as they are extremely resilient to hyperparameter choices and require little preprocessing. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
