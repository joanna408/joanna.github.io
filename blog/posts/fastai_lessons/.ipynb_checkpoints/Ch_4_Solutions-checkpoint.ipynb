{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Title: Chapter 4 Solutions\n",
    "\n",
    "Date: April 24, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "    - <span style=\"color:blue\">A grayscale image is represented by a list of NumPy arrays or PyTorch tensors. White pixels are stored as the number 0, black is the number 255 and shades of grey are between the two. For color images, separate red, green and blue components are stored for each pixel so the pixel value is a vector of 3 numbers. </span>\n",
    "\n",
    "2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n",
    "    - <span style=\"color:blue\">The MNIST dataset contains separate folders for training and validation set. Within those folders are the respective folders for each number or labels in the dataset. </span>\n",
    "3. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "    - <span style=\"color:blue\">For the pixel similarity approach, we take the average pixel values of all the training set digits (i.e. the 3's, the 7's) and we use each group average as the \"ideal\" representatin of a given digit. To classify a new digit image, we would calculate its distance from the ideal digit using the mean absolute difference or the root mean squared error. If the digit has the smallest distance from the ideal 3, it would be classified as a 3. If it has the smallest distance from the ideal 7, it would be classified as a 7 and so forth. </span>\n",
    "4. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "    - <span style=\"color:blue\">A list comprehension consists three parts: 1) the collection you are iterating over, 2) an optional filter and 3) something to do to each element. It's shorter to write and more efficient that creating the same list with a loop. \n",
    "double_odds = [2*x for x in a_list if x%2 !=0]</span> \n",
    "\n",
    "5. What is a \"rank-3 tensor\"?\n",
    "    - <span style=\"color:blue\">The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3. </span> \n",
    "\n",
    "6. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "    - <span style=\"color:blue\">Tensor rank is the number of dimensions or axes, and the tensor shape is the length of the axes. To get the rank of a tensor, use tensor.ndim() or len(tensor.shape).</span> \n",
    "\n",
    "7. What are RMSE and L1 norm?\n",
    "    - <span style=\"color:blue\">There are two commonly used methods of measuring “distance”. They provide the magnitudes of the differences that is needed to properly measure distances. RMSE: Square root of mean squared of differences, L1 norm: the mean of the absolute value of differences </span>\n",
    "    - <span style=\"color:blue\">With a_3 as the average pixel values of a given image and mean3 as the \"ideal\" 3:\n",
    "            - l1 = (a_3-mean3).abs().mean()\n",
    "            - l1 = ((a_3-mean3)**2)).mean().sqrt()\n",
    "\n",
    "8. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "    - <span style=\"color:blue\">Using NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure python as it is likely to be a wrapper for a compiled object written and optimized in another language, such as C. </span>\n",
    "9. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "    -    data = [[1,2,3],[4,5,6],[7,8,9]]\n",
    "    \n",
    "    tns = tensor(data)\n",
    "    \n",
    "    tns = tns*2\n",
    "\n",
    "    tns[1:,1:]\n",
    "    \n",
    "10. What is broadcasting?\n",
    "    - <span style=\"color:blue\">Broadcasting is a nice capability for tensor code that autoamtically expands the tensor with a smaller rank to have the same size as the one with the larger rank. </span>\n",
    "11. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "    - <span style=\"color:blue\">Metrics are generally calculated using the validation set because it tells us how well the model predicts on unseen data.</span>\n",
    "    \n",
    "12. What is SGD?\n",
    "    - <span style=\"color:blue\">SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.</span>\n",
    "    \n",
    "13. Why does SGD use mini-batches?\n",
    "    - <span style=\"color:blue\">To take an optimization step (change or update weights based on the gradients), we need to calculate loss over one or more data items. Rather than calculating it for the whole dataset which would take a long time, or to calculate for a single data item (which results in an imprecise and unstable gradient given its based on a single item), we calculate the average loss for a few data items at a time called a mini batch. Usiing mini-batches also allows us to utilize on an accelerator such as a GPU. </span>\n",
    "    \n",
    "14. What are the seven steps in SGD for machine learning?\n",
    "    1) <span style=\"color:blue\"> Initialize the weights  </span>\n",
    "    2) <span style=\"color:blue\"> Make predictions using given weights </span>\n",
    "    3) <span style=\"color:blue\"> Calculate loss to determine how good model performs with given weights </span>\n",
    "    4) <span style=\"color:blue\"> Calculate gradient to determine which direction to change the weights to minimize loss and set learning rate to decide step size </span>\n",
    "    5) <span style=\"color:blue\"> Change all the weights based on that calculation </span>\n",
    "    6) <span style=\"color:blue\"> Repeat the process from step 2 </span>\n",
    "    7) <span style=\"color:blue\"> Stop when epochs are reached or desired model performance is achieved </span>\n",
    "    \n",
    "15. How do we initialize the weights in a model?\n",
    "    - <span style=\"color:blue\">You can randomly intialize weights or use weights from pretrained model. </span>\n",
    "16. What is \"loss\"?\n",
    "    - <span style=\"color:blue\">A value based on a prediction and a target, where lower values of the function correspond to \"better\" predictions. For continuous data, its common to use mean squared error. </span>\n",
    "17. Why can't we always use a high learning rate?\n",
    "    - <span style=\"color:blue\">Using a high learning rate can result in the loss getting worse and taking many steps to train successfully where loss and parameters values diverge. </span>\n",
    "18. What is a \"gradient\"?\n",
    "    - <span style=\"color:blue\">The gradient tells us how much we have to change each weight to make our model better (and minimize loss). The gradient is defined as rise over run; that is the change in the value of the function divided by the change in the value of the parameter. By knowing how our function will change, we know which direction to adjust our parameters. If the gradient is very large, that may suggest that we have more adjustments to do, whereas if the gradient is very small, that may suggest that we are close to the optimal value.</span>\n",
    "19. Do you need to know how to calculate gradients yourself?\n",
    "    - <span style=\"color:blue\">No, PyTorch is able to automatically compute the derivatives of nearly any function. Given f(x) = x**2: </span>\n",
    "        \n",
    "            xt =tensor(3.).requires_grad_() # this tells PyTroch that we want to calculate gradients with respect to that variable at that value\n",
    "            yt = f(xt) # outputs tensor(9., grad_fn=\\<PowBackward0\\>)\n",
    "            yt.backward() # calculate the gradient\n",
    "            xt.grad # outputs tensor(6.)\n",
    "20. Why can't we use accuracy as a loss function?\n",
    "    - <span style=\"color:blue\">The gradient of a function is its slope that tell us how much the value of the function goes up or down divided by how much we changed the input. Accuracy only has a significant change when a prediction changes from one side to another. A small change in weights isn't likely to cause any prediction to change so the gradient is 0 almost everywhere. If we used accuracy as a loss function, the model will not be able to learn from that number given it will be 0 most of the time. </span>\n",
    "21. Draw the sigmoid function. What is special about its shape?\n",
    "    - <span style=\"color:blue\">Its outputs are always a number between 0 and 1. </span>\n",
    "\n",
    "22. What is the difference between a loss function and a metric?\n",
    "    - <span style=\"color:blue\">Metric is to drive human understanding and loss function is to drive automated learning. The loss must be a function with a meaningful derivative and is reasonably smooth. Metrics are values that tell us how our model is doing. </span>\n",
    "\n",
    "23. What is the function to calculate new weights using a learning rate?\n",
    "    - <span style=\"color:blue\">We use the magnitude of the gradient (steepness of the slope) to tell us how big a step to take. We multiply the gradient by a number we choose called the learning rate to decide on the step size. WE then iterate until we have reached the lowest point. </span>\n",
    "\n",
    "24. What does the DataLoader class do?\n",
    "    - <span style=\"color:blue\">It can take any Python collection and turn it into an iterator over many batches. When we pass a Dataset to a DataLoader, we get back many batches that are tuples of tensors representing batches of independent and dependent variables. </span>\n",
    "25. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "    for x,y in dl:\n",
    "        pred = model(x)\n",
    "        loss = loss_func(pred, y)\n",
    "        loss.backward()\n",
    "        parameters -= parameters.grad * lr\n",
    "        \n",
    "26. Create a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?\n",
    "    - <span style=\"color:blue\"> def func(a,b): return list(zip(a,b))</span>\n",
    "        This data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label.\n",
    "        \n",
    "27. What does view do in PyTorch?\n",
    "    - <span style=\"color:blue\">PyTorch method that changes the shape of a tensor without changing its contents.</span>\n",
    "28. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "    - <span style=\"color:blue\"> Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model. </span>\n",
    "29. What does the @ operator do in Python?\n",
    "    - <span style=\"color:blue\">Used for matrix multiplication</span>\n",
    "    \n",
    "30. What does the backward method do?\n",
    "    - <span style=\"color:blue\"> \"Backward\" refers to backpropagation, which is the process of calculating the derivative of each layer. </span>\n",
    "31. Why do we have to zero the gradients?\n",
    "    - <span style=\"color:blue\">loss.backward adds the gradients of loss to any gradients that are currently stored. Therefore we have to set the current gradients to 0 at the end of each epoch. </span>\n",
    "32. What information do we have to pass to Learner?\n",
    "    - <span style=\"color:blue\">To create a Learner, we need to pass in the DataLoaders, the model, the optimization function (which will be given the parameters), the loss function and optionally any metrics to print. </span>\n",
    "    \n",
    "33. Show Python or pseudocode for the basic steps of a training loop.\n",
    "    def train_epoch(model, lr, params):\n",
    "        for xb,yb in dl:\n",
    "            calc_grad(xb, yb, model)\n",
    "            for p in params:\n",
    "                p.data -= p.grad*lr\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    for i in range(20):\n",
    "        train_epoch(model, lr, params)\n",
    "    \n",
    "34. What is \"ReLU\"? Draw a plot of it for values from -2 to +2.\n",
    "    - <span style=\"color:blue\">A ReLU, or rectified linear unit, is a function that replaces every negative number with a zero: res.max(tensor(0.0)). It is also available in PyTorch as F.relu. </span>\n",
    "    \n",
    "35. What is an \"activation function\"?\n",
    "    - <span style=\"color:blue\">A nonlinearity that's different from ax+b</span>\n",
    "36. What's the difference between F.relu and nn.ReLU?\n",
    "    - <span style=\"color:blue\">nn.ReLU is a PyTorch module that does exactly the same thing as F.relu function. When using nn.Sequential, PyTorch requires us to use the module version. Note that modules are classes so we have to instantiate them. </span>\n",
    "37. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "    - <span style=\"color:blue\">With a deeper model (one with more layers), we do not need to use as many parameters. It turns out that we can use smaller matrices, with more layers, and get better results than we would get with larger matrices and few layers. This allows us to train the model more quickly and it also takes up less memory. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
