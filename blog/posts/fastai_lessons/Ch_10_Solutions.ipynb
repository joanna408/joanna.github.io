{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Title: Chapter 10 Solutions\n",
    "\n",
    "Date: April 27, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is \"self-supervised learning\"?\n",
    "    - <span style=\"color:blue\">Self-supervising learning is when training a model where the labels are embedded in the independent variable, rather than requiring external labels. </span>\n",
    "2. What is a \"language model\"?\n",
    "    - <span style=\"color:blue\">A language model is a model that has been trained to guess the next word in a text after reading the ones before. </span>\n",
    "3. Why is a language model considered self-supervised?\n",
    "    - <span style=\"color:blue\">We do not give labels to our model. Instead we feed it lots of texts and it is able to automatically get the labels from the data. </span>\n",
    "4. What are self-supervised models usually used for?\n",
    "    - <span style=\"color:blue\">These models are usually used for pretraining a model used for transfer learning. </span>\n",
    "5. Why do we fine-tune language models?\n",
    "    - <span style=\"color:blue\">While the pretrained model may know the basics of the language we are using in the task, it helps to get used to the style of the corpus we are targeting. In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia. </span>\n",
    "6. What are the three steps to create a state-of-the-art text classifier?\n",
    "    - <span style=\"color:blue\">1)  The LM is trained on a general-domain corpus to capture\n",
    "general features of the language in different layers. 2) Fine-tune the LM on data of the target task so it adapts to the idiosyncracies of the target data. 3) Fine-tune the LM as a text classifier. </span>\n",
    "7. How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n",
    "    - <span style=\"color:blue\">By fine-tuning the pretraining language model (already trained on Wikipedia articles), this LM will become particularly good at predicting the next word of a movie review. For it to improve in predicting the next word of a text, it will need a better understanding of the language style and structure of the text the IMDb dataset, which will in turn help it perform better as a text classifier. </span>\n",
    "\n",
    "8. What are the three steps to prepare your data for a language model?\n",
    "    - <span style=\"color:blue\">Tokenization, numericalization and language model data loader </span>\n",
    "9. What is \"tokenization\"? Why do we need it?\n",
    "    - <span style=\"color:blue\">A deep learning model expects numbers as inputs, not English words so we need to convert the text into a list of words through tokenization. </span>\n",
    "10. Name three different approaches to tokenization.\n",
    "    - <span style=\"color:blue\">1) Word-based: split a sentence on spaces and applying language-specific rules to try to separate parts of meaning even when there are no spaces (i.e. \"don't\" into \"do n't\") 2) Subword based: split words into samller parts 3) split a sentence into its individual characters </span>\n",
    "11. What is `xxbos`?\n",
    "    - <span style=\"color:blue\">A special token that indicates the start of a new text. </span>\n",
    "12. List four rules that fastai applies to text during tokenization.\n",
    "    1) <span style=\"color:blue\"> A sequence of the same character will be replaced with a special repeated character token (xxrep), followed by the number of times the character is repeated and then the single character (i.e. !!!! > 'xxrep', '4', '!'  </span>\n",
    "    2) <span style=\"color:blue\"> A capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word (i.e. INDEX > 'xxup', 'index') </span>\n",
    "    3) <span style=\"color:blue\"> xxmaj indicates the next word begins with a capital (since we lowercased everything) (i.e. 'This movie...' > 'xxmaj', 'this', 'movie') </span>\n",
    "    4) <span style=\"color:blue\"> xxunk indicates a word is unknown </span>\n",
    "13. Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?\n",
    "    - <span style=\"color:blue\">This allows the model to encode information about repeated characters without requiring a separate token for every repetition of the same character. </span>\n",
    "14. What is \"numericalization\"?\n",
    "    - <span style=\"color:blue\">The process of mapping tokens to integers </span>\n",
    "15. Why might there be words that are replaced with the \"unknown word\" token?\n",
    "    - <span style=\"color:blue\">Max_vocab of 60000 is one of the defaults to Numericalize. Therefore, all words other than the most common 60,000 will be replaced with a special unknown word token. This avoids having an overly large embedding matrix, since that can slow down trainign and use up too much memory. The second default is min_freq=3 which means any words that appears fewer than 3 times is replaced with xxunk as well.  </span>\n",
    "16. With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Carefulâ€”students often get this one wrong! Be sure to check your answer on the book's website.)\n",
    "    - <span style=\"color:blue\">With a batch size of 64 asnd a sequence length of 8, the second row of that tensor will have tokens 9-16. The first row of the second batch contains tokens 65-73. </span>\n",
    "17. Why do we need padding for text classification? Why don't we need it for language modeling?\n",
    "    - <span style=\"color:blue\">PyTorch DataLoaders collects all items in a batch into a single tensor but each tensor has to be a fixed shape. To make sure all the texts are the same length, we will expand the shortest texts to make them all the same size. We don't have the same issue with language modeling since we concatenate all the documents first and then split them into equally sized sections. </span>\n",
    "18. What does an embedding matrix for NLP contain? What is its shape?\n",
    "    - <span style=\"color:blue\">It contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. </span>\n",
    "19. What is \"perplexity\"?\n",
    "    - <span style=\"color:blue\">It is the expoential of the loss, often used as a metric in NLP for language models. </span>\n",
    "20. Why do we have to pass the vocabulary of the language model to the classifier data block?\n",
    "    - <span style=\"color:blue\">By passing the vocab of the language model, we make sure we are using the same correspondence of token to index. Otherwise, the embeddings we learned in our fine-tuned language model won't make any sense to the model and the fine-tuning step will no longer be useful. </span>\n",
    "21. What is \"gradual unfreezing\"?\n",
    "    - <span style=\"color:blue\">This refers to unfreezing one layer at a time and fine-tuning the pretrained model.\n",
    "\n",
    " </span>\n",
    "22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts?\n",
    "    - <span style=\"color:blue\">Because a better classification or identification algorithm can be used to create an even better generation algorithm. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
