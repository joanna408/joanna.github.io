{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP \n",
    "\n",
    "For the week of 4/24/23, I spent my time on Chapter 4 (Lecture 3) and Chapter 10 (Lecture 4) of FastAi. There were 37 questions as the end of Chapter 4 so it took a couple days to get through it. The first half of the week was focused on how to create a basic digit classifier and the workings of stochastic gradient descent. The second half of the week was spent on Lecture 4 where Jeremey introduces how to train a NLP model using HuggingFace Transformers. \n",
    "\n",
    "I thought about diving into an NLP Kaggle competition but the example we're shown is merely one task of the many types of NLP projects. Therefore, I've decided for this first pass through of the course, rather than feeling utterly lost when presented with a Kaggle competition, I'd like to read through popular notebooks from various competitions from the last year and jot down notes of their approach. Through this exposure, I will hopefully gain a better understanding of how to tackle various NLP tasks from preprocessing and EDA to building existing models and improving on it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "### [Source: Jeremy Howard's NLP For Absolute Beginners](https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners/) and [Iterate like a Grandmaster](https://www.kaggle.com/code/jhoward/iterate-like-a-grandmaster/)\n",
    "- Competition:  U.S. Patent Phrase to Phrase Matching\n",
    "- Task: Compare two words or short phrases and score them based on whether they're similar to not. Score of 1 means two inputs are identical in meaning, 0 means they have totally different meanings\n",
    "- Input format: 'TEXT 1: A47; TEXT 2: abatement of pollution; ANC1: abatement'\n",
    "- Preprocessing: Combining three columns into one string of text, Tokenization and numericalization\n",
    "- Model: Microsoft Deberta V3 Small\n",
    "- Metric: Pearson coefficient between predicted and actual similarity scores\n",
    "- EDA: look at distribution of each input column (histogram of scores, value counts by column) to make sure training and validation sets are comparable\n",
    "- Ways to improve baseline model: \n",
    "    - What if we made the patent section a special token? Then potentially the model might learn to recognize that different sections need to be handled in different ways.\n",
    "    - Try a model pretrained on legal vocabulary. E.g. how about BERT for patents?\n",
    "    - You'd likely get better results by using a sentence similarity model. Did you know that there's a patent similarity model you could try?\n",
    "    - You could also fine-tune any HuggingFace model using the full patent database (which is provided in BigQuery), before applying it to this dataset\n",
    "- Libraries: datasetes, transformers\n",
    "[Notes from 4/28/23]\n",
    "\n",
    "### [Source: Fares Sayah NLP For Beginners](https://www.kaggle.com/code/faressayah/natural-language-processing-nlp-for-beginners)\n",
    "- Task: Spam Detection\n",
    "- Input format: TF-IDF document-term matrix\n",
    "- Preprocessing: \n",
    "    - Remove all punctuation, all stopwords\n",
    "    - Vectorization with CountVectorizer (converts text into a matrix of token counts), use TFIDF Transformer to multiply term frequency with inverse document frequency (weigh the counts so that frequent tokens get lower weight) \n",
    "- Model: Multinomial Naive Bayes, Logistic Regression\n",
    "- Metric: Accuracy\n",
    "- EDA: message length, common words \n",
    "- Ways to improve baseline model: \n",
    "    - Vectorizer Turning - use language specific stopwords lists, use varying n_gram lengths, ignore terms that have document frequency higher or below a specific threshold\n",
    "- Libraries: scikitlearn, nltk\n",
    "\n",
    "[Notes from 4/29/23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/nkitgupta/text-representations\n",
    "https://www.kaggle.com/code/aishwarya2210/prediction-of-tweets-using-bert-model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
