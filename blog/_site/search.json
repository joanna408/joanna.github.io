[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "posts/4_21_23_Post_1.html",
    "href": "posts/4_21_23_Post_1.html",
    "title": "My fast.ai plan",
    "section": "",
    "text": "It’s been a couple weeks since I started my fast.ai journey. I’ve been hesitant on starting a blog because of of the amount of work it seems to require. Also, because I’m not sure what I’d actually write about. But I’ve decided to give it a shot today. Rather than aiming for perfection on the first try, my goal is to do what what I can, then return and improve.\nIn Lesson 0, Jeremy Howard explains the best approach to getting the most out of the fast.ai course. The key takeaways for me were 1) finish the course 2) complete a personal project 3) write about it and 4) share your work. Additionally, for each lesson, he recommends watching the lecture, running and experimenting with the notebooks, familiarize yourself with the code and rewrite and implement everything for your own project. Thinking about doing all these recommended steps is daunting.\nTherefore, instead of aiming for perfection on the first try, I will be aiming for completion. Below is my current plan for this first runthrough of the course: 1. Lessons: - Watch the lecture, read and run through the notebook, answer the chapter questions, familiarize myself with the code by writing down the code and commenting out each line’s function. 2. Personal Project: - Find a Kaggle competition that interests me. 3. Blog: - Write a post at the end of every week. Don’t get bogged down by stylizing, or creating an actual website. The important part is tracking learnings, progress and obstacles. 4. Share Your Work: - Hold until second runthrough."
  },
  {
    "objectID": "posts/4_21_23_Post_1.html#my-progress",
    "href": "posts/4_21_23_Post_1.html#my-progress",
    "title": "My fast.ai plan",
    "section": "My progress",
    "text": "My progress\nCurrently, I am finishing up the reading for Lesson 4 (I skipped Lesson 3 on Data Ethics for now). Below is my progress so far: - For lesson 1, I ran the code on search images of golden doodles versus fried chicken. - For lesson 2, I ran the code on search images of cake and bread and created a simple Cake or Bread app on Hugging Face: https://huggingface.co/spaces/joannah/cake_or_bread The model is pretty terrible given it’s only trained on a total of 45 images."
  },
  {
    "objectID": "posts/4_21_23_Post_1.html#key-concepts",
    "href": "posts/4_21_23_Post_1.html#key-concepts",
    "title": "My fast.ai plan",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nWhat is machine learning?\n\n\nWhat is a neural network?\n\n\nWhat is stochastic gradient descent?"
  },
  {
    "objectID": "posts/fastai_lessons/Ch_4_Solutions.html",
    "href": "posts/fastai_lessons/Ch_4_Solutions.html",
    "title": "Title: Chapter 4 Solutions",
    "section": "",
    "text": "Date: April 24, 2023\n\n\nHow is a grayscale image represented on a computer? How about a color image?\n\nA grayscale image is represented by a list of NumPy arrays or PyTorch tensors. White pixels are stored as the number 0, black is the number 255 and shades of grey are between the two. For color images, separate red, green and blue components are stored for each pixel so the pixel value is a vector of 3 numbers. \n\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n\nThe MNIST dataset contains separate folders for training and validation set. Within those folders are the respective folders for each number or labels in the dataset. \n\nExplain how the “pixel similarity” approach to classifying digits works.\n\nFor the pixel similarity approach, we take the average pixel values of all the training set digits (i.e. the 3’s, the 7’s) and we use each group average as the “ideal” representatin of a given digit. To classify a new digit image, we would calculate its distance from the ideal digit using the mean absolute difference or the root mean squared error. If the digit has the smallest distance from the ideal 3, it would be classified as a 3. If it has the smallest distance from the ideal 7, it would be classified as a 7 and so forth. \n\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n\nA list comprehension consists three parts: 1) the collection you are iterating over, 2) an optional filter and 3) something to do to each element. It’s shorter to write and more efficient that creating the same list with a loop. double_odds = [2*x for x in a_list if x%2 !=0]\n\nWhat is a “rank-3 tensor”?\n\nThe rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3. \n\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\n\nTensor rank is the number of dimensions or axes, and the tensor shape is the length of the axes. To get the rank of a tensor, use tensor.ndim() or len(tensor.shape).\n\nWhat are RMSE and L1 norm?\n\nThere are two commonly used methods of measuring “distance”. They provide the magnitudes of the differences that is needed to properly measure distances. RMSE: Square root of mean squared of differences, L1 norm: the mean of the absolute value of differences \nWith a_3 as the average pixel values of a given image and mean3 as the “ideal” 3: - l1 = (a_3-mean3).abs().mean() - l1 = ((a_3-mean3)**2)).mean().sqrt()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n\nUsing NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure python as it is likely to be a wrapper for a compiled object written and optimized in another language, such as C. \n\nCreate a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\ndata = [[1,2,3],[4,5,6],[7,8,9]]\n\ntns = tensor(data)\ntns = tns*2\ntns[1:,1:]\nWhat is broadcasting?\n\nBroadcasting is a nice capability for tensor code that autoamtically expands the tensor with a smaller rank to have the same size as the one with the larger rank. \n\nAre metrics generally calculated using the training set, or the validation set? Why?\n\nMetrics are generally calculated using the validation set because it tells us how well the model predicts on unseen data.\n\nWhat is SGD?\n\nSGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\n\nWhy does SGD use mini-batches?\n\nTo take an optimization step (change or update weights based on the gradients), we need to calculate loss over one or more data items. Rather than calculating it for the whole dataset which would take a long time, or to calculate for a single data item (which results in an imprecise and unstable gradient given its based on a single item), we calculate the average loss for a few data items at a time called a mini batch. Usiing mini-batches also allows us to utilize on an accelerator such as a GPU. \n\nWhat are the seven steps in SGD for machine learning?\n\n Initialize the weights \n Make predictions using given weights \n Calculate loss to determine how good model performs with given weights \n Calculate gradient to determine which direction to change the weights to minimize loss and set learning rate to decide step size \n Change all the weights based on that calculation \n Repeat the process from step 2 \n Stop when epochs are reached or desired model performance is achieved \n\nHow do we initialize the weights in a model?\n\nYou can randomly intialize weights or use weights from pretrained model. \n\nWhat is “loss”?\n\nA value based on a prediction and a target, where lower values of the function correspond to “better” predictions. For continuous data, its common to use mean squared error. \n\nWhy can’t we always use a high learning rate?\n\nUsing a high learning rate can result in the loss getting worse and taking many steps to train successfully where loss and parameters values diverge. \n\nWhat is a “gradient”?\n\nThe gradient tells us how much we have to change each weight to make our model better (and minimize loss). The gradient is defined as rise over run; that is the change in the value of the function divided by the change in the value of the parameter. By knowing how our function will change, we know which direction to adjust our parameters. If the gradient is very large, that may suggest that we have more adjustments to do, whereas if the gradient is very small, that may suggest that we are close to the optimal value.\n\nDo you need to know how to calculate gradients yourself?\n\nNo, PyTorch is able to automatically compute the derivatives of nearly any function. Given f(x) = x**2: \n  xt =tensor(3.).requires_grad_() # this tells PyTroch that we want to calculate gradients with respect to that variable at that value\n  yt = f(xt) # outputs tensor(9., grad_fn=\\&lt;PowBackward0\\&gt;)\n  yt.backward() # calculate the gradient\n  xt.grad # outputs tensor(6.)\n\nWhy can’t we use accuracy as a loss function?\n\nThe gradient of a function is its slope that tell us how much the value of the function goes up or down divided by how much we changed the input. Accuracy only has a significant change when a prediction changes from one side to another. A small change in weights isn’t likely to cause any prediction to change so the gradient is 0 almost everywhere. If we used accuracy as a loss function, the model will not be able to learn from that number given it will be 0 most of the time. \n\nDraw the sigmoid function. What is special about its shape?\n\nIts outputs are always a number between 0 and 1. \n\nWhat is the difference between a loss function and a metric?\n\nMetric is to drive human understanding and loss function is to drive automated learning. The loss must be a function with a meaningful derivative and is reasonably smooth. Metrics are values that tell us how our model is doing. \n\nWhat is the function to calculate new weights using a learning rate?\n\nWe use the magnitude of the gradient (steepness of the slope) to tell us how big a step to take. We multiply the gradient by a number we choose called the learning rate to decide on the step size. WE then iterate until we have reached the lowest point. \n\nWhat does the DataLoader class do?\n\nIt can take any Python collection and turn it into an iterator over many batches. When we pass a Dataset to a DataLoader, we get back many batches that are tuples of tensors representing batches of independent and dependent variables. \n\nWrite pseudocode showing the basic steps taken in each epoch for SGD. for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr\nCreate a function that, if passed two arguments [1,2,3,4] and ‘abcd’, returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)]. What is special about that output data structure?\n\n def func(a,b): return list(zip(a,b)) This data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label.\n\nWhat does view do in PyTorch?\n\nPyTorch method that changes the shape of a tensor without changing its contents.\n\nWhat are the “bias” parameters in a neural network? Why do we need them?\n\n Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model. \n\nWhat does the @ operator do in Python?\n\nUsed for matrix multiplication\n\nWhat does the backward method do?\n\n “Backward” refers to backpropagation, which is the process of calculating the derivative of each layer. \n\nWhy do we have to zero the gradients?\n\nloss.backward adds the gradients of loss to any gradients that are currently stored. Therefore we have to set the current gradients to 0 at the end of each epoch. \n\nWhat information do we have to pass to Learner?\n\nTo create a Learner, we need to pass in the DataLoaders, the model, the optimization function (which will be given the parameters), the loss function and optionally any metrics to print. \n\nShow Python or pseudocode for the basic steps of a training loop. def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_()\nfor i in range(20): train_epoch(model, lr, params)\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\n\nA ReLU, or rectified linear unit, is a function that replaces every negative number with a zero: res.max(tensor(0.0)). It is also available in PyTorch as F.relu. \n\nWhat is an “activation function”?\n\nA nonlinearity that’s different from ax+b\n\nWhat’s the difference between F.relu and nn.ReLU?\n\nnn.ReLU is a PyTorch module that does exactly the same thing as F.relu function. When using nn.Sequential, PyTorch requires us to use the module version. Note that modules are classes so we have to instantiate them. \n\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n\nWith a deeper model (one with more layers), we do not need to use as many parameters. It turns out that we can use smaller matrices, with more layers, and get better results than we would get with larger matrices and few layers. This allows us to train the model more quickly and it also takes up less memory."
  }
]