[
  {
    "objectID": "posts/progress/4_21_23_Post_1.html",
    "href": "posts/progress/4_21_23_Post_1.html",
    "title": "My fast.ai plan",
    "section": "",
    "text": "It’s been a couple weeks since I started my fast.ai journey. I’ve been hesitant on starting a blog because of of the amount of work it seems to require. Also, because I’m not sure what I’d actually write about. But I’ve decided to give it a shot today. Rather than aiming for perfection on the first try, my goal is to do what what I can, then return and improve.\nIn Lesson 0, Jeremy Howard explains the best approach to getting the most out of the fast.ai course. The key takeaways for me were 1) finish the course 2) complete a personal project 3) write about it and 4) share your work. Additionally, for each lesson, he recommends watching the lecture, running and experimenting with the notebooks, familiarize yourself with the code and rewrite and implement everything for your own project. Thinking about doing all these recommended steps is daunting.\nTherefore, instead of aiming for perfection on the first try, I will be aiming for completion. Below is my current plan for this first runthrough of the course: 1. Lessons: - Watch the lecture, read and run through the notebook, answer the chapter questions, familiarize myself with the code by writing down the code and commenting out each line’s function. 2. Personal Project: - Find a Kaggle competition that interests me. 3. Blog: - Write a post at the end of every week. Don’t get bogged down by stylizing, or creating an actual website. The important part is tracking learnings, progress and obstacles. 4. Share Your Work: - Hold until second runthrough."
  },
  {
    "objectID": "posts/progress/4_21_23_Post_1.html#my-progress",
    "href": "posts/progress/4_21_23_Post_1.html#my-progress",
    "title": "My fast.ai plan",
    "section": "My progress",
    "text": "My progress\nCurrently, I am finishing up the reading for Lesson 4 (I skipped Lesson 3 on Data Ethics for now). Below is my progress so far: - For lesson 1, I ran the code on search images of golden doodles versus fried chicken. - For lesson 2, I ran the code on search images of cake and bread and created a simple Cake or Bread app on Hugging Face: https://huggingface.co/spaces/joannah/cake_or_bread The model is pretty terrible given it’s only trained on a total of 45 images."
  },
  {
    "objectID": "posts/progress/4_21_23_Post_1.html#key-concepts",
    "href": "posts/progress/4_21_23_Post_1.html#key-concepts",
    "title": "My fast.ai plan",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nWhat is machine learning?\n\nMachine learning, like regular programming, is a way to get computers to complete a task. Normally, we just think about the exact steps we’d take to complete the task and then we translate them into code. But for some tasks, defining exact steps can bit tricky, since it’s all happening in our brain without any conscious effort. Therefore, instead of telling the computer the exact steps for solving a problem, how can we have it figure it out by itself? For example, imagine we’re trying to predict pastry sales for a cafe. We believe that factors such as weather and day of the week are influential factors. We can assign weights to these variables. Depending on the weights we assign, our model will put more emphasis on the variable with a higher weight. We can initialize these weights to any random number at first then calculate how our predictions differ from historical sales records. Based on the difference we calculate, we adjust the weights a little further in the direction that is a closer fit. This process can be entirely automatic and a machine can be programmed to “learn” from its experience. \n\n\n\nWhat is a neural network?\n\nA neural network is a type of machine learning. It is a mathematical function that is extremely flexible and can solve a wide range of problems just by finding the right weights. \n\n\n\nWhat is stochastic gradient descent?\n\nGradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function. To make gradient descent more efficient on large sets of data, we utilize stochastic gradient descent which randomly picks one data point from the whole data set to calculate the derivative at each iteration."
  },
  {
    "objectID": "posts/progress/NLP.html",
    "href": "posts/progress/NLP.html",
    "title": "NLP",
    "section": "",
    "text": "For the week of 4/24/23, I spent my time on Chapter 4 (Lecture 3) and Chapter 10 (Lecture 4) of FastAi. There were 37 questions as the end of Chapter 4 so it took a couple days to get through it. The first half of the week was focused on how to create a basic digit classifier and the workings of stochastic gradient descent. The second half of the week was spent on Lecture 4 where Jeremey introduces how to train a NLP model using HuggingFace Transformers.\nI thought about diving into an NLP Kaggle competition but the example we’re shown is merely one task of the many types of NLP projects. Therefore, I’ve decided for this first pass through of the course, rather than feeling utterly lost when presented with a Kaggle competition, I’d like to read through popular notebooks from various competitions from the last year and jot down notes of their approach. Through this exposure, I will hopefully gain a better understanding of how to tackle various NLP tasks from preprocessing and EDA to building existing models and improving on it."
  },
  {
    "objectID": "posts/progress/NLP.html#text-classification",
    "href": "posts/progress/NLP.html#text-classification",
    "title": "NLP",
    "section": "Text Classification",
    "text": "Text Classification\n\nSource: Jeremy Howard’s NLP For Absolute Beginners and Iterate like a Grandmaster\n\nCompetition: U.S. Patent Phrase to Phrase Matching\nTask: Compare two words or short phrases and score them based on whether they’re similar to not. Score of 1 means two inputs are identical in meaning, 0 means they have totally different meanings\nInput format: ‘TEXT 1: A47; TEXT 2: abatement of pollution; ANC1: abatement’\nPreprocessing: Combining three columns into one string of text, Tokenization and numericalization\nModel: Microsoft Deberta V3 Small\nMetric: Pearson coefficient between predicted and actual similarity scores\nEDA: look at distribution of each input column (histogram of scores, value counts by column) to make sure training and validation sets are comparable\nWays to improve baseline model:\n\nWhat if we made the patent section a special token? Then potentially the model might learn to recognize that different sections need to be handled in different ways.\nTry a model pretrained on legal vocabulary. E.g. how about BERT for patents?\nYou’d likely get better results by using a sentence similarity model. Did you know that there’s a patent similarity model you could try?\nYou could also fine-tune any HuggingFace model using the full patent database (which is provided in BigQuery), before applying it to this dataset\n\nLibraries: datasetes, transformers [Notes from 4/28/23]\n\n\n\nSource: Fares Sayah NLP For Beginners\n\nTask: Spam Detection\nInput format: TF-IDF document-term matrix\nPreprocessing:\n\nRemove all punctuation, all stopwords\nVectorization with CountVectorizer (converts text into a matrix of token counts), use TFIDF Transformer to multiply term frequency with inverse document frequency (weigh the counts so that frequent tokens get lower weight)\n\nModel: Multinomial Naive Bayes, Logistic Regression\nMetric: Accuracy\nEDA: message length, common words\nWays to improve baseline model:\n\nVectorizer Turning - use language specific stopwords lists, use varying n_gram lengths, ignore terms that have document frequency higher or below a specific threshold\n\nLibraries: scikitlearn, nltk\n\n[Notes from 4/29/23]\nhttps://www.kaggle.com/code/nkitgupta/text-representations https://www.kaggle.com/code/aishwarya2210/prediction-of-tweets-using-bert-model"
  },
  {
    "objectID": "posts/fastai_lessons/Ch_9_Solutions.html",
    "href": "posts/fastai_lessons/Ch_9_Solutions.html",
    "title": "Title: Chapter 9 Solutions",
    "section": "",
    "text": "Date: May 10, 2023\n\n\nWhat is a continuous variable?\n\nContinuous variables are numerical data, such as “age”, that can be directly fed to the model since you can add and multiply them directly. \n\nWhat is a categorical variable?\n\nCategorical variables contain a number of discrete levels, such as “movie ID,” for which addition and multiplication don’t have meaning even if they’re stored as numbers. \n\nProvide two of the words that are used for the possible values of a categorical variable.\n\nPossible values of a categorical variable can be called its “levels”, “categories” or “classes.” \n\nWhat is a “dense layer”?\n\nThe “dense layer” is equivalent to an ordinary linear layer that’s placed after every one-hot-encoded input layer. \n\nHow do entity embeddings reduce memory usage and speed up neural networks?\n\nWhen datasets have high cardinality features, it can be memory intensive to store them as one hot encodings that are likely sparse. Entity embeddings allow more memory efficient and dense representations of the data which therefore helps speed up neural networks. \n\nWhat kinds of datasets are entity embeddings especially useful for?\n\nEntity embeddings are especially useful for datasets with lots of high cardinality features where other methods tend to overfit. It is also helpful for visualizing categorial data and for data clustering as the entity embeddings define a distance measure for categorical variables.\n\nWhat are the two main families of machine learning algorithms?\n\nEnsembles of decision trees which are mainly used for structured data and multilayered neural networks learned with stochastic gradient descent, mainly used for unstructured data such as audio, images and natural language \n\nWhy do some categorical columns need a special ordering in their classes? How do you do this in Pandas?\n\nThese type of columns are called ordinal columns which refers to strings that have a natural ordering such as “small”, “medium”, “large.” To do this in Pandas, we first set the dataframe column astype ‘category.’ Then we pass in an ordered list to set_categories with ordered set to True to let Pandas know about a suitable ordering of the column levels. \n\nSummarize what a decision tree algorithm does.\n\nThe basic idea of what a decision tree algorithm does is to determine how to group the data based on “questions” that we ask about the data. That is, we keep splitting the data based on the levels or values of the features and generate predictions based on the average target value of the data points in that group. Here is the algorithm:\n\n - Loop through each column of the dataset in turn   - For each column, loop through each possible level of that column in turn   - Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable)   - Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple “model” where our predictions are simply the average sale price of the item’s group   - After looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model   - We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group   - Continue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it. \nWhy is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model?\n\nSome dates are qualitatively different from others. We might want our model to make decisions based on that date’s day of the week, on whether a day is a holiday, on what month it is in and so forth. Therefore, we replace every date column with a set of date metadata columnstjat we think will be useful. \n\nShould you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick?\n\nNo, as the goal of the model is to predict the future. Therefore we will need a validation set to be later in time than the training set. \n\nWhat is pickle and what is it useful for?\n\nIt allows you to save nearly any Python object. For example, since preprocessing datat can take some time, we can save our preprocessed TabularPandas object at a given point so tis easy to continue our work another day without rerunning previous steps. \n\nHow are mse, samples, and values calculated in the decision tree drawn in this chapter?\n\nBy traversing the tree based on answering questions about the data, we reach the nodes that tell us the average value of the data in that group, the mse, and the number of samples in that group. \n\nHow do we deal with outliers, before building a decision tree?\n\nOutliers can sometimes make visualizing decision trees more difficult. For example, with year 1000 as a placeholder for missing values, we weren’t able to get a good visualization of the distribution of the YearMade data. By changing the values to 1950, the distribution was much more apparent through the tree visualization; it also doesn’t change the result of the model in any significant way which demonstrates the resilience of decision trees to data issues. \n\nHow do we handle categorical variables in a decision tree?\n\nWe don’t have to do anything. During training, the decision tree algorithm will automatically make splits that increase homogenity within a subgroup, which hones in on the intrinsic meaning of the different levels of the variable. \n\nWhat is bagging?\n\nBagging is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The process is as follows: 1) Randomly choose a subset of the rows of your data 2) Train a model using this subset 3) Save that model, and tehn return to step 1 a few times 4) You will end up with multiple trained models. To make a prediction, predict using all of the models and then take the average of each of those model’s predictions. While each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will be uncorrelated with each other. Different models will make different errors and the average of those errors is zero. Therefore if we take the average of all the model’s predictions, we should end up with a prediction that gets closer and closer to the correct answer, the more models we have. \n\nWhat is the difference between max_samples and max_features when creating a random forest?\n\nMax_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point. \n\nIf you increase n_estimators to a very high value, can that lead to overfitting? Why or why not?\n\nx \n\nIn the section “Creating a Random Forest”, just after &lt;&gt;, why did preds.mean(0) give the same result as our random forest?\n\nBecause we are taking the average of the predictions from each individual tree in our forest through the estimators_ attribute.\n\nWhat is “out-of-bag-error”?\n\nA way of measuring prediction error in the training dataset by validating only on rows that were not included in the training \n\nMake a list of reasons why a model’s validation set error might be worse than the OOB error. How could you test your hypotheses?\n\nx \n\nExplain why random forests are well suited to answering each of the following question:\n\nHow confident are we in our predictions using a particular row of data? Measure standard deviation of predictions across the trees; we would want to be more cautious of using results for rows where trees give very different results (higher standard dev) compared to cases where they are more consistent (lower standard dev) \nFor predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? Use the feature_importances_ attribute which loops through each tree and recursively explores each branch looking at what feature was used for that split, and how much the model improves as a result of that split \nWhich columns are the strongest predictors? Features with the highest importance scores. \nHow do predictions vary as we vary these columns? Partial dependence plots allows us to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? \n\nWhat’s the purpose of removing unimportant variables?\n\nSimpler, more interpretable models are easier to roll out and maintain. By honing in on the important variables, we will also be able to study them more in depth. \n\nWhat’s a good type of plot for showing tree interpreter results?\n\nA waterfall plot shows how the positive and negative contributions from all the independent variables sum up to create the final prediction. \n\nWhat is the “extrapolation problem”?\n\nRandom forests are not able to extrapolate outside of the types of data they have seen. \n\nHow can you tell if your test or validation set is distributed in a different way than your training set?\n\nWe can combine our training and validation sets, create a dependent variable that represents which dataset each row comes from, build a random forest using that data and get its feature importance. By looking at the feature importance, we can see which columns differ significantly between the datasets. \n\nWhy do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?\n\nThis is a variable that changes over time, and since we want our model to extrapolate for future results, we make this a continuous variable. \n\nWhat is “boosting”?\n\nWe train a model that underfits the dataset, and train subsequent models that predicts the error of the original model. Each new tree will be attempting to fit the error of all of the previous trees combined. We continually create new resideuals by subtracting the predictions of each new tree from the residuals of the previous trees. As a result, the residuals increasingly decrease. To make predictions with an ensemple of boosted trees, we calculate the predictions from each tree and then add them all together. \n\nHow could we use embeddings with a random forest? Would we expect this to help?\n\nEntity embeddings contains richer representations of the categorical features and definitely can improve the performance of other models like random forests. Instead of passing in the raw categorical columns, the entity embeddings can be passed into the random forest model. \n\nWhy might we not always use a neural net for tabular modeling?\n\nNeural networsk take the longest time to train and require extra preprocessing such as normalization. They are also highly sensitive to hyperparameter settings. Therefore it is best to start with a random forest as they are extremely resilient to hyperparameter choices and require little preprocessing."
  },
  {
    "objectID": "posts/fastai_lessons/Ch_4_Solutions.html",
    "href": "posts/fastai_lessons/Ch_4_Solutions.html",
    "title": "Title: Chapter 4 Solutions",
    "section": "",
    "text": "Date: April 24, 2023\n\n\nHow is a grayscale image represented on a computer? How about a color image?\n\nA grayscale image is represented by a list of NumPy arrays or PyTorch tensors. White pixels are stored as the number 0, black is the number 255 and shades of grey are between the two. For color images, separate red, green and blue components are stored for each pixel so the pixel value is a vector of 3 numbers. \n\nHow are the files and folders in the MNIST_SAMPLE dataset structured? Why?\n\nThe MNIST dataset contains separate folders for training and validation set. Within those folders are the respective folders for each number or labels in the dataset. \n\nExplain how the “pixel similarity” approach to classifying digits works.\n\nFor the pixel similarity approach, we take the average pixel values of all the training set digits (i.e. the 3’s, the 7’s) and we use each group average as the “ideal” representatin of a given digit. To classify a new digit image, we would calculate its distance from the ideal digit using the mean absolute difference or the root mean squared error. If the digit has the smallest distance from the ideal 3, it would be classified as a 3. If it has the smallest distance from the ideal 7, it would be classified as a 7 and so forth. \n\nWhat is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n\nA list comprehension consists three parts: 1) the collection you are iterating over, 2) an optional filter and 3) something to do to each element. It’s shorter to write and more efficient that creating the same list with a loop. double_odds = [2*x for x in a_list if x%2 !=0]\n\nWhat is a “rank-3 tensor”?\n\nThe rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3. \n\nWhat is the difference between tensor rank and shape? How do you get the rank from the shape?\n\nTensor rank is the number of dimensions or axes, and the tensor shape is the length of the axes. To get the rank of a tensor, use tensor.ndim() or len(tensor.shape).\n\nWhat are RMSE and L1 norm?\n\nThere are two commonly used methods of measuring “distance”. They provide the magnitudes of the differences that is needed to properly measure distances. RMSE: Square root of mean squared of differences, L1 norm: the mean of the absolute value of differences \nWith a_3 as the average pixel values of a given image and mean3 as the “ideal” 3: - l1 = (a_3-mean3).abs().mean() - l1 = ((a_3-mean3)**2)).mean().sqrt()\n\nHow can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n\nUsing NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure python as it is likely to be a wrapper for a compiled object written and optimized in another language, such as C. \n\nCreate a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n\ndata = [[1,2,3],[4,5,6],[7,8,9]]\n\ntns = tensor(data)\ntns = tns*2\ntns[1:,1:]\nWhat is broadcasting?\n\nBroadcasting is a nice capability for tensor code that autoamtically expands the tensor with a smaller rank to have the same size as the one with the larger rank. \n\nAre metrics generally calculated using the training set, or the validation set? Why?\n\nMetrics are generally calculated using the validation set because it tells us how well the model predicts on unseen data.\n\nWhat is SGD?\n\nSGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\n\nWhy does SGD use mini-batches?\n\nTo take an optimization step (change or update weights based on the gradients), we need to calculate loss over one or more data items. Rather than calculating it for the whole dataset which would take a long time, or to calculate for a single data item (which results in an imprecise and unstable gradient given its based on a single item), we calculate the average loss for a few data items at a time called a mini batch. Usiing mini-batches also allows us to utilize on an accelerator such as a GPU. \n\nWhat are the seven steps in SGD for machine learning?\n\n Initialize the weights \n Make predictions using given weights \n Calculate loss to determine how good model performs with given weights \n Calculate gradient to determine which direction to change the weights to minimize loss and set learning rate to decide step size \n Change all the weights based on that calculation \n Repeat the process from step 2 \n Stop when epochs are reached or desired model performance is achieved \n\nHow do we initialize the weights in a model?\n\nYou can randomly intialize weights or use weights from pretrained model. \n\nWhat is “loss”?\n\nA value based on a prediction and a target, where lower values of the function correspond to “better” predictions. For continuous data, its common to use mean squared error. \n\nWhy can’t we always use a high learning rate?\n\nUsing a high learning rate can result in the loss getting worse and taking many steps to train successfully where loss and parameters values diverge. \n\nWhat is a “gradient”?\n\nThe gradient tells us how much we have to change each weight to make our model better (and minimize loss). The gradient is defined as rise over run; that is the change in the value of the function divided by the change in the value of the parameter. By knowing how our function will change, we know which direction to adjust our parameters. If the gradient is very large, that may suggest that we have more adjustments to do, whereas if the gradient is very small, that may suggest that we are close to the optimal value.\n\nDo you need to know how to calculate gradients yourself?\n\nNo, PyTorch is able to automatically compute the derivatives of nearly any function. Given f(x) = x**2: \n  xt =tensor(3.).requires_grad_() # this tells PyTroch that we want to calculate gradients with respect to that variable at that value\n  yt = f(xt) # outputs tensor(9., grad_fn=\\&lt;PowBackward0\\&gt;)\n  yt.backward() # calculate the gradient\n  xt.grad # outputs tensor(6.)\n\nWhy can’t we use accuracy as a loss function?\n\nThe gradient of a function is its slope that tell us how much the value of the function goes up or down divided by how much we changed the input. Accuracy only has a significant change when a prediction changes from one side to another. A small change in weights isn’t likely to cause any prediction to change so the gradient is 0 almost everywhere. If we used accuracy as a loss function, the model will not be able to learn from that number given it will be 0 most of the time. \n\nDraw the sigmoid function. What is special about its shape?\n\nIts outputs are always a number between 0 and 1. \n\nWhat is the difference between a loss function and a metric?\n\nMetric is to drive human understanding and loss function is to drive automated learning. The loss must be a function with a meaningful derivative and is reasonably smooth. Metrics are values that tell us how our model is doing. \n\nWhat is the function to calculate new weights using a learning rate?\n\nWe use the magnitude of the gradient (steepness of the slope) to tell us how big a step to take. We multiply the gradient by a number we choose called the learning rate to decide on the step size. WE then iterate until we have reached the lowest point. \n\nWhat does the DataLoader class do?\n\nIt can take any Python collection and turn it into an iterator over many batches. When we pass a Dataset to a DataLoader, we get back many batches that are tuples of tensors representing batches of independent and dependent variables. \n\nWrite pseudocode showing the basic steps taken in each epoch for SGD. for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr\nCreate a function that, if passed two arguments [1,2,3,4] and ‘abcd’, returns [(1, ‘a’), (2, ‘b’), (3, ‘c’), (4, ‘d’)]. What is special about that output data structure?\n\n def func(a,b): return list(zip(a,b)) This data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label.\n\nWhat does view do in PyTorch?\n\nPyTorch method that changes the shape of a tensor without changing its contents.\n\nWhat are the “bias” parameters in a neural network? Why do we need them?\n\n Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model. \n\nWhat does the @ operator do in Python?\n\nUsed for matrix multiplication\n\nWhat does the backward method do?\n\n “Backward” refers to backpropagation, which is the process of calculating the derivative of each layer. \n\nWhy do we have to zero the gradients?\n\nloss.backward adds the gradients of loss to any gradients that are currently stored. Therefore we have to set the current gradients to 0 at the end of each epoch. \n\nWhat information do we have to pass to Learner?\n\nTo create a Learner, we need to pass in the DataLoaders, the model, the optimization function (which will be given the parameters), the loss function and optionally any metrics to print. \n\nShow Python or pseudocode for the basic steps of a training loop. def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_()\nfor i in range(20): train_epoch(model, lr, params)\nWhat is “ReLU”? Draw a plot of it for values from -2 to +2.\n\nA ReLU, or rectified linear unit, is a function that replaces every negative number with a zero: res.max(tensor(0.0)). It is also available in PyTorch as F.relu. \n\nWhat is an “activation function”?\n\nA nonlinearity that’s different from ax+b\n\nWhat’s the difference between F.relu and nn.ReLU?\n\nnn.ReLU is a PyTorch module that does exactly the same thing as F.relu function. When using nn.Sequential, PyTorch requires us to use the module version. Note that modules are classes so we have to instantiate them. \n\nThe universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n\nWith a deeper model (one with more layers), we do not need to use as many parameters. It turns out that we can use smaller matrices, with more layers, and get better results than we would get with larger matrices and few layers. This allows us to train the model more quickly and it also takes up less memory."
  },
  {
    "objectID": "posts/fastai_lessons/Ch_10_Solutions.html",
    "href": "posts/fastai_lessons/Ch_10_Solutions.html",
    "title": "Title: Chapter 10 Solutions",
    "section": "",
    "text": "Date: April 27, 2023\n\n\nWhat is “self-supervised learning”?\n\nSelf-supervising learning is when training a model where the labels are embedded in the independent variable, rather than requiring external labels. \n\nWhat is a “language model”?\n\nA language model is a model that has been trained to guess the next word in a text after reading the ones before. \n\nWhy is a language model considered self-supervised?\n\nWe do not give labels to our model. Instead we feed it lots of texts and it is able to automatically get the labels from the data. \n\nWhat are self-supervised models usually used for?\n\nThese models are usually used for pretraining a model used for transfer learning. \n\nWhy do we fine-tune language models?\n\nWhile the pretrained model may know the basics of the language we are using in the task, it helps to get used to the style of the corpus we are targeting. In the case of the IMDb dataset, there will be lots of names of movie directors and actors, and often a less formal style of language than that seen in Wikipedia. \n\nWhat are the three steps to create a state-of-the-art text classifier?\n\n1) The LM is trained on a general-domain corpus to capture general features of the language in different layers. 2) Fine-tune the LM on data of the target task so it adapts to the idiosyncracies of the target data. 3) Fine-tune the LM as a text classifier. \n\nHow do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?\n\nBy fine-tuning the pretraining language model (already trained on Wikipedia articles), this LM will become particularly good at predicting the next word of a movie review. For it to improve in predicting the next word of a text, it will need a better understanding of the language style and structure of the text the IMDb dataset, which will in turn help it perform better as a text classifier. \n\nWhat are the three steps to prepare your data for a language model?\n\nTokenization, numericalization and language model data loader \n\nWhat is “tokenization”? Why do we need it?\n\nA deep learning model expects numbers as inputs, not English words so we need to convert the text into a list of words through tokenization. \n\nName three different approaches to tokenization.\n\n1) Word-based: split a sentence on spaces and applying language-specific rules to try to separate parts of meaning even when there are no spaces (i.e. “don’t” into “do n’t”) 2) Subword based: split words into samller parts 3) split a sentence into its individual characters \n\nWhat is xxbos?\n\nA special token that indicates the start of a new text. \n\nList four rules that fastai applies to text during tokenization.\n\n A sequence of the same character will be replaced with a special repeated character token (xxrep), followed by the number of times the character is repeated and then the single character (i.e. !!!! &gt; ‘xxrep’, ‘4’, ‘!’ \n A capitalized word will be replaced with a special capitalization token, followed by the lowercase version of the word (i.e. INDEX &gt; ‘xxup’, ‘index’) \n xxmaj indicates the next word begins with a capital (since we lowercased everything) (i.e. ‘This movie…’ &gt; ‘xxmaj’, ‘this’, ‘movie’) \n xxunk indicates a word is unknown \n\nWhy are repeated characters replaced with a token showing the number of repetitions and the character that’s repeated?\n\nThis allows the model to encode information about repeated characters without requiring a separate token for every repetition of the same character. \n\nWhat is “numericalization”?\n\nThe process of mapping tokens to integers \n\nWhy might there be words that are replaced with the “unknown word” token?\n\nMax_vocab of 60000 is one of the defaults to Numericalize. Therefore, all words other than the most common 60,000 will be replaced with a special unknown word token. This avoids having an overly large embedding matrix, since that can slow down trainign and use up too much memory. The second default is min_freq=3 which means any words that appears fewer than 3 times is replaced with xxunk as well. \n\nWith a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer on the book’s website.)\n\nWith a batch size of 64 asnd a sequence length of 8, the second row of that tensor will have tokens 9-16. The first row of the second batch contains tokens 65-73. \n\nWhy do we need padding for text classification? Why don’t we need it for language modeling?\n\nPyTorch DataLoaders collects all items in a batch into a single tensor but each tensor has to be a fixed shape. To make sure all the texts are the same length, we will expand the shortest texts to make them all the same size. We don’t have the same issue with language modeling since we concatenate all the documents first and then split them into equally sized sections. \n\nWhat does an embedding matrix for NLP contain? What is its shape?\n\nIt contains vector representations of all tokens in the vocabulary. The embedding matrix has the size (vocab_size x embedding_size), where vocab_size is the length of the vocabulary, and embedding_size is an arbitrary number defining the number of latent factors of the tokens. \n\nWhat is “perplexity”?\n\nIt is the expoential of the loss, often used as a metric in NLP for language models. \n\nWhy do we have to pass the vocabulary of the language model to the classifier data block?\n\nBy passing the vocab of the language model, we make sure we are using the same correspondence of token to index. Otherwise, the embeddings we learned in our fine-tuned language model won’t make any sense to the model and the fine-tuning step will no longer be useful. \n\nWhat is “gradual unfreezing”?\n\nThis refers to unfreezing one layer at a time and fine-tuning the pretrained model.\n\n\n 22. Why is text generation always likely to be ahead of automatic identification of machine-generated texts? - Because a better classification or identification algorithm can be used to create an even better generation algorithm."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]